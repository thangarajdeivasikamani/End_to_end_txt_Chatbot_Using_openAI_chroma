# -*- coding: utf-8 -*-
"""Chroma_DB_Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ecuWMAK4U_PWR0vagoJ5f7KtAQoHvYeV
"""

# tiktoken dependcy with open AI & chroma DB
#pip install chromadb openai tiktoken langchain

#pip show langchain

# download the data
#wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip

#unzip -q new_articles.zip -d new_articles

from dotenv import load_dotenv
import os 

load_dotenv()

OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")

# from google.colab import userdata
# OPENAI_API_KEY = userdata.get('OPENAPI_KEY')

# import os
# os.environ["OPENAI_API_KEY"]=OPENAI_API_KEY

# import chroma from langchain framwork
# import open AI embedding from lanhchain framwork
#from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores import Chroma
#from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_openai import OpenAI
# import directory loader to data from directoty
# import text loader becasue we are working with text
from langchain_community.document_loaders import DirectoryLoader
#from langchain.document_loaders import DirectoryLoader
#from langchain.document_loaders import TextLoader
from langchain_community.document_loaders import TextLoader

text_loader_kwargs={'autodetect_encoding': True} # utf-8 encoding 
loader= DirectoryLoader("data",glob="./*.txt",loader_cls=TextLoader,loader_kwargs=text_loader_kwargs)
Docuemnts = loader.load()
print(len(Docuemnts))
# we can use differnt different laoder to load various format like CSV,pdf,json & finally convert into docuemnts

# # All the data in the loader will be extracted and loaded into documents variable
# Docuemnts = loader.load()

# # How many file that much docuemnts are there
# Docuemnts

# print(len(Docuemnts))

# Now if you see your docuemnt token size is more than 4096 - So we can't use directly or pass to model
# So use chunking



from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

chunks = text_splitter.split_documents(Docuemnts)

# see the chunk size
len(chunks)

chunks[1]

# Create vector DB
persist_directory="chroma_db"

embedding = OpenAIEmbeddings()

embedding

vectordb = Chroma.from_documents(documents =chunks, embedding=embedding, persist_directory="chroma_db")

#persistent the DB to disk
vectordb.persist()
vectordb = None

# /content/chroma_db folder we can see chromadb sqlite

# Now load the DB
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)

"""## Sementic search"""

retriever = vectordb.as_retriever(search_kwargs={"k":2})

docs = retriever.get_relevant_documents("How much money did Microsoft raise?")

# now we can only 2 response
len(docs)

#what type of search
retriever.search_type

#How many word
retriever.search_kwargs

"""## make chain"""

# create the chain to answer question
# here we are using RetrievalQA chain
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
)

#full example
query = "How much money did Microsoft raise?"

llm_response = qa_chain(query)

llm_response

# print source info
def process_llm_response(llm_response):
  print("----------------------------------------")
  print(llm_response['result'])
  print("----------------------------------------")
  print('\n\nSources:')
  for source in llm_response["source_documents"]:
    print(source.metadata['source'])

process_llm_response(llm_response)



